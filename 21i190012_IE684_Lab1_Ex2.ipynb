{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21i190012_IE684_Lab1_Ex2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 1. Exercise 2. }$\n",
        "\n",
        "Now we will consider a slightly different algorithm which can be used to find a minimizer of the function $f(\\mathbf{x})=f(x_1,x_2)= (x_1+100)^2 + (x_2-25)^2$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gpe6eGRLvSh"
      },
      "source": [
        "$\\textbf{[R]}$ Write the function $f(\\mathbf{x})$ in the form $\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{x}\\in {\\mathbb{R}}^2$, $\\mathbf{A}$ is a symmetric matrix of size $2 \\times 2$, $\\mathbf{b}\\in{\\mathbb{R}}^2$ and $c\\in\\mathbb{R}$. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTPeLBt0L7F7"
      },
      "source": [
        "#Ans 1:\n",
        "Write your answer here:\n",
        "\n",
        "$\\begin{bmatrix} x_1 & x_2 \\end{bmatrix}\\begin{bmatrix} 1 & 0 \\\\ 0 & 1\\end{bmatrix}\\begin{bmatrix} x_1 \\\\x_2 \\end{bmatrix} + 2\\begin{bmatrix} 100 & -25 \\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix} + 10625$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjANnIQ3L39D"
      },
      "source": [
        "\n",
        "$\\textbf{[R]}$ It turns out that for a function $f:{\\mathbb{R}}^n\\rightarrow \\mathbb{R}$ of the form $f(\\mathbf{x})=\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{A}\\in{\\mathbb{R}}^{n \\times n}$ is a symmetric matrix, $\\mathbf{b} \\in {\\mathbb{R}}^n$ and $c\\in \\mathbb{R}$, the analytical solution to $\\min_{\\alpha \\geq 0} f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))$ can be found in closed form. Find the solution. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jU-adJ0L-P1"
      },
      "source": [
        "#Ans 2:\n",
        "Write your answer here:\n",
        "\n",
        "We can see that $\\nabla f(\\mathbf{x}) = 2(\\mathbf{A}\\mathbf{x}+\\mathbf{b})$\n",
        "\n",
        "We substitute $\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x})$ in $f(\\mathbf{x})=\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$ and then compute the derivative of the resultant with respect to $\\alpha$.\n",
        "\n",
        "On equating the resultant to zero we can get the required solution : \n",
        "\n",
        "$\\alpha = [\\ (\\ \\mathbf{x} + \\mathbf{b} (\\ \\mathbf{A}^{-1} )\\ ^{T} )\\ . \\mathbf{T_1}^{T} ]\\ . \\mathbf{T_2}^{-1}$\n",
        "\n",
        "where\n",
        "\n",
        "$\\mathbf{T_1} = 2 (\\ \\mathbf{A}\\mathbf{x}+\\mathbf{b}) \\\\ \\mathbf{T_2} = \\mathbf{T_1}.\\mathbf{T_1}^{T}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "We will use this idea to construct a suitable step length finding procedure for our modified algorithm given below: \n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$}  \\\\\n",
        "& \\textbf{Initialize } k=0 \\\\ \n",
        "&\\textbf{While } \\| \\nabla f(\\mathbf{x}^k) \\|_2 > \\tau \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\eta^k = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k - \\eta  \\nabla f(\\mathbf{x}^k)) \\\\\n",
        "&\\quad \\quad \\mathbf{x}^{k+1} \\leftarrow \\mathbf{x}^k - \\eta^k \\nabla f(\\mathbf{x}^k)  \\\\ \n",
        "&\\quad \\quad k = {k+1} \\\\ \n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\mathbf{x}^k\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at https://numpy.org/doc/stable/ for numpy documentation\n",
        "#we will first import the numpy package and name it as np\n",
        "import numpy as np \n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  return (x[0]+100)**2 + (x[1]-25)**2\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  return np.array([2*(x[0]+100),2*(x[1]-25)])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength\n",
        "def compute_steplength(start_x): #add appropriate arguments to the function \n",
        "  #Complete the code\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2\n",
        "  A = np.identity(2)\n",
        "  b = np.array([100,-25])\n",
        "  t1 = np.add(2*np.matmul(A,start_x),2*b)\n",
        "  t2 = np.matmul(t1,t1.transpose())\n",
        "\n",
        "  step_length = np.matmul(np.add(x,np.matmul(b,np.linalg.inv(A).transpose())),t1.transpose())/t2\n",
        "\n",
        "  return step_length"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer(start_x, tol):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  k = 0\n",
        "  print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    step_length = compute_steplength(x) #call the new function you wrote to compute the steplength\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x \n"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-kHCkbwe-M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c68cba40-624e-4114-aea6-45919586e312"
      },
      "source": [
        "my_start_x = np.array([10,10])\n",
        "my_tol= 1e-3\n",
        "find_minimizer(my_start_x, my_tol)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-100.,   25.])"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer(start_x, tol):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    step_length = compute_steplength(x) #call the new function you wrote to compute the steplength\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return k, x \n"
      ],
      "metadata": {
        "id": "E4eTREYCmoiy"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft_3BxMzfREx"
      },
      "source": [
        "no_of_iterations = []\n",
        "tolerance = []\n",
        "for i in range(1,11):\n",
        "  my_tol = 10**-i\n",
        "  k, opt = find_minimizer(my_start_x, my_tol)\n",
        "  tolerance.append(my_tol)\n",
        "  no_of_iterations.append(k)"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print('For tolerance =',tolerance[i],'\\nNo. of iterations :',no_of_iterations[i],'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBxFedJknCq8",
        "outputId": "8a8bd231-7458-4797-a640-5040f897a9ac"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For tolerance = 0.1 \n",
            "No. of iterations : 1 \n",
            "\n",
            "For tolerance = 0.01 \n",
            "No. of iterations : 1 \n",
            "\n",
            "For tolerance = 0.001 \n",
            "No. of iterations : 1 \n",
            "\n",
            "For tolerance = 0.0001 \n",
            "No. of iterations : 1 \n",
            "\n",
            "For tolerance = 1e-05 \n",
            "No. of iterations : 1 \n",
            "\n",
            "For tolerance = 1e-06 \n",
            "No. of iterations : 1 \n",
            "\n",
            "For tolerance = 1e-07 \n",
            "No. of iterations : 1 \n",
            "\n",
            "For tolerance = 1e-08 \n",
            "No. of iterations : 1 \n",
            "\n",
            "For tolerance = 1e-09 \n",
            "No. of iterations : 1 \n",
            "\n",
            "For tolerance = 1e-10 \n",
            "No. of iterations : 1 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ans 4:\n",
        "\n",
        "We can observe in this case the no. of iterations taken is always 1 irrespective of the change in tolerance which is in total contrast to the case when step length was constant."
      ],
      "metadata": {
        "id": "cbg2SkO0nrJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(tolerance, no_of_iterations)\n",
        "plt.xlabel('Tolerance')\n",
        "plt.ylabel('No. of iterations')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "UxBRXSROnTRh",
        "outputId": "f1a6bd16-aa78-4158-fcc8-5a4e5764f09c"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'No. of iterations')"
            ]
          },
          "metadata": {},
          "execution_count": 122
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYJaACtKMIGH"
      },
      "source": [
        "${\\Large\\text{Do not forget to rename the file before submission.}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcsCIAntMNdb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}