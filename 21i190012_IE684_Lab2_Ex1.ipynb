{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21i190012_IE684_Lab2_Ex1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 2. Exercise 1. }$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "Recall that we implemented the gradient descent algorithm to solve $\\min_{\\mathbf{x} \\in {\\mathbb{R}}^n} f(\\mathbf{x})$. The main ingredients in the gradient descent iterations are the descent direction $\\mathbf{p}^k$ which is set to $-\\nabla f(\\mathbf{x}^k)$, and the step length $\\eta^k$ which is found by solving an optimization problem (or sometimes taken as a constant value over all iterations). We used the following procedure in the previous lab:\n",
        "\n",
        "\\begin{align}\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$}  \\\\\n",
        "& \\textbf{Initialize } k=0 \\\\ \n",
        "& \\mathbf{p}^k =-\\nabla f(\\mathbf{x}^k) \\\\ \n",
        "&\\textbf{While } \\| \\mathbf{p}^k \\|_2 > \\tau \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\eta^k = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k + \\eta  \\mathbf{p}^k) = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k - \\eta  \\nabla f(\\mathbf{x}^k)) \\\\\n",
        "&\\quad \\quad \\mathbf{x}^{k+1} = \\mathbf{x}^k + \\eta^k \\mathbf{p}^k = \\mathbf{x}^k - \\eta^k \\nabla f (\\mathbf{x}^k)  \\\\ \n",
        "&\\quad \\quad k = {k+1} \\\\ \n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\mathbf{x}^k\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ivDCuJRP9b"
      },
      "source": [
        "We saw that for particular cases of quadratic functions, a closed form analytical solution for the minimizer of the optimization problem $\\min_{\\eta \\geq 0} f({\\mathbf{x}}^k + \\eta {\\mathbf{p}}^k)$ exists. However finding a closed form expression as a solution to this optimization problem to find a suitable step length might not always be possible. To tackle general situations, we will try to devise a different procedure in this lab. \n",
        "\n",
        "To find the step length, we will use the following property: \n",
        "Suppose a non-zero $\\mathbf{p} \\in {\\mathbb{R}}^n$ is a descent direction at point $\\mathbf{x}$, and let $\\gamma \\in (0,1)$. Then there exists $\\varepsilon >0$ such that  \n",
        "\\begin{align}\n",
        "f(\\mathbf{x}+\\alpha \\mathbf{p}) \\leq f(\\mathbf{x}) + \\gamma \\alpha \\nabla f(\\mathbf{x})^\\top \\mathbf{p}, \\ \\forall \\alpha \\in (0,\\varepsilon].  \n",
        "\\end{align}\n",
        "\n",
        "The step length $\\eta^k$ can be found using a backtracking procedure illustrated below to find appropriate value of $\\varepsilon$.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6OddaNAmpA"
      },
      "source": [
        "\n",
        "\\begin{align}\n",
        "& \\textbf{Input:}  \\text{ $\\mathbf{x}^k$, $\\mathbf{p}^k$, $\\alpha^0$, $\\rho \\in (0,1)$, $\\gamma \\in (0,1)$ }  \\\\\n",
        "& \\textbf{Initialize } \\alpha=\\alpha^0 \\\\ \n",
        "&\\textbf{While } f(\\mathbf{x}^k + \\alpha \\mathbf{p}^k)   > f(\\mathbf{x}^k) + \\gamma \\alpha \\nabla f(\\mathbf{x}^k)^\\top \\mathbf{p}^k \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\alpha = \\rho \\alpha  \\\\\n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\alpha\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW-xcDISWmGR"
      },
      "source": [
        "In this exercise, we will check if finding the steplength using the backtracking procedure is advantageous for some quadratic functions. In this sample code we consider $f(\\mathbf{x})=f(x_1,x_2) = (x_1-8)^2 + (x_2 + 12)^2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at Please have a look at https://numpy.org/doc/stable/ for numpy documentation\n",
        "#we will first import the numpy package and name it as np\n",
        "import numpy as np \n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  #compute the function value and return it \n",
        "  return (x[1]+12)**2 + (-8+x[0])**2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  #compute the gradient value and return it \n",
        "  return np.array([2*(x[0]-8), 2*(x[1]+12)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\n",
        "def compute_steplength_exact(gradf, A): #add appropriate arguments to the function \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(A) is np.ndarray and A.shape[0] == 2 and  A.shape[1] == 2 #allow only a 2x2 array\n",
        "\n",
        "  t1 = np.matmul(gradf,gradf)/2\n",
        "  t2 = np.matmul(np.matmul(A,gradf),gradf)\n",
        "\n",
        "  step_length = t1/t2\n",
        "  \n",
        "  return step_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGunDYy6Q21S"
      },
      "source": [
        "#Complete the module to compute the steplength by using the backtracking line search\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(x) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  \n",
        "  alpha = alpha_start\n",
        "  #implement the backtracking line search\n",
        "\n",
        "  while evalf(x+alpha*(-gradf)) > evalf(x) + gamma*alpha*np.matmul(gradf.transpose(),-gradf):\n",
        "    alpha = rho*alpha\n",
        "\n",
        "\n",
        "  #print('final step length:',alpha)\n",
        "  return alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaUUdzLtVSCl"
      },
      "source": [
        "#we define the types of line search methods that we have implemented\n",
        "EXACT_LINE_SEARCH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer(start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  # construct a suitable A matrix for the quadratic function \n",
        "  A = np.array([[1, 0],[0,1]])\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "\n",
        "    print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-kHCkbwe-M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71d98cb-2588-4c3c-f9eb-5b14c4f12c38"
      },
      "source": [
        "my_start_x = np.array([1,1])\n",
        "my_tol= 1e-5\n",
        "\n",
        "\n",
        "x_opt = find_minimizer(my_start_x, my_tol, CONSTANT_STEP_LENGTH)\n",
        "print(x_opt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0  x: [1 1]  f(x): 218  grad at x: [-14  26]  gradient norm: 29.5296461204668\n",
            "iter: 1  x: [ 2.4 -1.6]  f(x): 139.52  grad at x: [-11.2  20.8]  gradient norm: 23.62371689637344\n",
            "iter: 2  x: [ 3.52 -3.68]  f(x): 89.2928  grad at x: [-8.96 16.64]  gradient norm: 18.898973517098753\n",
            "iter: 3  x: [ 4.416 -5.344]  f(x): 57.147391999999996  grad at x: [-7.168 13.312]  gradient norm: 15.119178813679001\n",
            "iter: 4  x: [ 5.1328 -6.6752]  f(x): 36.57433088  grad at x: [-5.7344 10.6496]  gradient norm: 12.095343050943201\n",
            "iter: 5  x: [ 5.70624 -7.74016]  f(x): 23.407571763199996  grad at x: [-4.58752  8.51968]  gradient norm: 9.676274440754561\n",
            "iter: 6  x: [ 6.164992 -8.592128]  f(x): 14.980845928447996  grad at x: [-3.670016  6.815744]  gradient norm: 7.741019552603648\n",
            "iter: 7  x: [ 6.5319936 -9.2737024]  f(x): 9.587741394206713  grad at x: [-2.9360128  5.4525952]  gradient norm: 6.192815642082918\n",
            "iter: 8  x: [ 6.82559488 -9.81896192]  f(x): 6.136154492292296  grad at x: [-2.34881024  4.36207616]  gradient norm: 4.954252513666333\n",
            "iter: 9  x: [  7.0604759  -10.25516954]  f(x): 3.9271388750670724  grad at x: [-1.87904819  3.48966093]  gradient norm: 3.963402010933068\n",
            "iter: 10  x: [  7.24838072 -10.60413563]  f(x): 2.5133688800429272  grad at x: [-1.50323855  2.79172874]  gradient norm: 3.1707216087464554\n",
            "iter: 11  x: [  7.39870458 -10.8833085 ]  f(x): 1.608556083227473  grad at x: [-1.20259084  2.23338299]  gradient norm: 2.5365772869971637\n",
            "iter: 12  x: [  7.51896366 -11.1066468 ]  f(x): 1.0294758932655836  grad at x: [-0.96207267  1.7867064 ]  gradient norm: 2.029261829597732\n",
            "iter: 13  x: [  7.61517093 -11.28531744]  f(x): 0.6588645716899738  grad at x: [-0.76965814  1.42936512]  gradient norm: 1.623409463678186\n",
            "iter: 14  x: [  7.69213674 -11.42825395]  f(x): 0.42167332588158263  grad at x: [-0.61572651  1.14349209]  gradient norm: 1.2987275709425479\n",
            "iter: 15  x: [  7.7537094  -11.54260316]  f(x): 0.2698709285642127  grad at x: [-0.49258121  0.91479367]  gradient norm: 1.0389820567540378\n",
            "iter: 16  x: [  7.80296752 -11.63408253]  f(x): 0.17271739428109584  grad at x: [-0.39406497  0.73183494]  gradient norm: 0.8311856454032296\n",
            "iter: 17  x: [  7.84237401 -11.70726602]  f(x): 0.11053913233990176  grad at x: [-0.31525197  0.58546795]  gradient norm: 0.664948516322585\n",
            "iter: 18  x: [  7.87389921 -11.76581282]  f(x): 0.07074504469753722  grad at x: [-0.25220158  0.46837436]  gradient norm: 0.5319588130580684\n",
            "iter: 19  x: [  7.89911937 -11.81265026]  f(x): 0.04527682860642402  grad at x: [-0.20176126  0.37469949]  gradient norm: 0.4255670504464556\n",
            "iter: 20  x: [  7.91929549 -11.8501202 ]  f(x): 0.028977170308111266  grad at x: [-0.16140901  0.29975959]  gradient norm: 0.34045364035716386\n",
            "iter: 21  x: [  7.9354364  -11.88009616]  f(x): 0.018545388997191298  grad at x: [-0.12912721  0.23980767]  gradient norm: 0.27236291228573173\n",
            "iter: 22  x: [  7.94834912 -11.90407693]  f(x): 0.011869048958202257  grad at x: [-0.10330177  0.19184614]  gradient norm: 0.21789032982858378\n",
            "iter: 23  x: [  7.95867929 -11.92326154]  f(x): 0.007596191333249553  grad at x: [-0.08264141  0.15347691]  gradient norm: 0.17431226386286827\n",
            "iter: 24  x: [  7.96694343 -11.93860924]  f(x): 0.004861562453279738  grad at x: [-0.06611313  0.12278153]  gradient norm: 0.13944981109029495\n",
            "iter: 25  x: [  7.97355475 -11.95088739]  f(x): 0.003111399970098944  grad at x: [-0.0528905   0.09822522]  gradient norm: 0.11155984887223437\n",
            "iter: 26  x: [  7.9788438  -11.96070991]  f(x): 0.001991295980863367  grad at x: [-0.0423124   0.07858018]  gradient norm: 0.08924787909778846\n",
            "iter: 27  x: [  7.98307504 -11.96856793]  f(x): 0.001274429427752583  grad at x: [-0.03384992  0.06286414]  gradient norm: 0.07139830327823156\n",
            "iter: 28  x: [  7.98646003 -11.97485434]  f(x): 0.0008156348337616257  grad at x: [-0.02707994  0.05029131]  gradient norm: 0.05711864262258429\n",
            "iter: 29  x: [  7.98916802 -11.97988347]  f(x): 0.000522006293607447  grad at x: [-0.02166395  0.04023305]  gradient norm: 0.045694914098067724\n",
            "iter: 30  x: [  7.99133442 -11.98390678]  f(x): 0.0003340840279087578  grad at x: [-0.01733116  0.03218644]  gradient norm: 0.03655593127845372\n",
            "iter: 31  x: [  7.99306754 -11.98712542]  f(x): 0.00021381377786159158  grad at x: [-0.01386493  0.02574915]  gradient norm: 0.029244745022762062\n",
            "iter: 32  x: [  7.99445403 -11.98970034]  f(x): 0.00013684081783143522  grad at x: [-0.01109194  0.02059932]  gradient norm: 0.023395796018211068\n",
            "iter: 33  x: [  7.99556322 -11.99176027]  f(x): 8.757812341210526e-05  grad at x: [-0.00887355  0.01647946]  gradient norm: 0.018716636814567436\n",
            "iter: 34  x: [  7.99645058 -11.99340822]  f(x): 5.6049998983743945e-05  grad at x: [-0.00709884  0.01318357]  gradient norm: 0.014973309451653491\n",
            "iter: 35  x: [  7.99716046 -11.99472657]  f(x): 3.587199934959814e-05  grad at x: [-0.00567907  0.01054685]  gradient norm: 0.011978647561323129\n",
            "iter: 36  x: [  7.99772837 -11.99578126]  f(x): 2.295807958373843e-05  grad at x: [-0.00454326  0.00843748]  gradient norm: 0.009582918049057589\n",
            "iter: 37  x: [  7.9981827  -11.99662501]  f(x): 1.469317093359739e-05  grad at x: [-0.00363461  0.00674999]  gradient norm: 0.007666334439247323\n",
            "iter: 38  x: [  7.99854616 -11.99730001]  f(x): 9.403629397501814e-06  grad at x: [-0.00290769  0.00539999]  gradient norm: 0.0061330675513976895\n",
            "iter: 39  x: [  7.99883693 -11.99784   ]  f(x): 6.018322814400039e-06  grad at x: [-0.00232615  0.00431999]  gradient norm: 0.0049064540411176945\n",
            "iter: 40  x: [  7.99906954 -11.998272  ]  f(x): 3.851726601214466e-06  grad at x: [-0.00186092  0.00345599]  gradient norm: 0.0039251632328933615\n",
            "iter: 41  x: [  7.99925563 -11.9986176 ]  f(x): 2.4651050247789585e-06  grad at x: [-0.00148874  0.00276479]  gradient norm: 0.003140130586315772\n",
            "iter: 42  x: [  7.99940451 -11.99889408]  f(x): 1.577667215859531e-06  grad at x: [-0.00119099  0.00221184]  gradient norm: 0.002512104469053412\n",
            "iter: 43  x: [  7.9995236  -11.99911527]  f(x): 1.0097070181511879e-06  grad at x: [-0.00095279  0.00176947]  gradient norm: 0.002009683575243812\n",
            "iter: 44  x: [  7.99961888 -11.99929221]  f(x): 6.462124916174952e-07  grad at x: [-0.00076223  0.00141557]  gradient norm: 0.0016077468601959641\n",
            "iter: 45  x: [  7.99969511 -11.99943377]  f(x): 4.135759946358159e-07  grad at x: [-0.00060979  0.00113246]  gradient norm: 0.0012861974881577338\n",
            "iter: 46  x: [  7.99975609 -11.99954702]  f(x): 2.646886365671574e-07  grad at x: [-0.00048783  0.00090597]  gradient norm: 0.001028957990526644\n",
            "iter: 47  x: [  7.99980487 -11.99963761]  f(x): 1.6940072740309958e-07  grad at x: [-0.00039026  0.00072477]  gradient norm: 0.0008231663924216041\n",
            "iter: 48  x: [  7.99984389 -11.99971009]  f(x): 1.084164655380788e-07  grad at x: [-0.00031221  0.00057982]  gradient norm: 0.000658533113937572\n",
            "iter: 49  x: [  7.99987512 -11.99976807]  f(x): 6.938653794453523e-08  grad at x: [-0.00024977  0.00046386]  gradient norm: 0.0005268264911506833\n",
            "iter: 50  x: [  7.99990009 -11.99981446]  f(x): 4.440738428483721e-08  grad at x: [-0.00019981  0.00037108]  gradient norm: 0.0004214611929221347\n",
            "iter: 51  x: [  7.99992007 -11.99985157]  f(x): 2.842072594202808e-08  grad at x: [-0.00015985  0.00029687]  gradient norm: 0.00033716895433611967\n",
            "iter: 52  x: [  7.99993606 -11.99988125]  f(x): 1.818926460302129e-08  grad at x: [-0.00012788  0.00023749]  gradient norm: 0.00026973516346981007\n",
            "iter: 53  x: [  7.99994885 -11.999905  ]  f(x): 1.164112934610497e-08  grad at x: [-0.00010231  0.00019   ]  gradient norm: 0.00021578813077743615\n",
            "iter: 54  x: [  7.99995908 -11.999924  ]  f(x): 7.450322781467721e-09  grad at x: [-8.18440917e-05  1.51996170e-04]  gradient norm: 0.00017263050462149173\n",
            "iter: 55  x: [  7.99996726 -11.9999392 ]  f(x): 4.7682065801825415e-09  grad at x: [-6.54752734e-05  1.21596936e-04]  gradient norm: 0.00013810440369781902\n",
            "iter: 56  x: [  7.99997381 -11.99995136]  f(x): 3.051652211238402e-09  grad at x: [-5.23802187e-05  9.72775490e-05]  gradient norm: 0.00011048352295683554\n",
            "iter: 57  x: [  7.99997905 -11.99996109]  f(x): 1.9530574151798167e-09  grad at x: [-4.19041749e-05  7.78220392e-05]  gradient norm: 8.838681836517969e-05\n",
            "iter: 58  x: [  7.99998324 -11.99996887]  f(x): 1.2499567457533644e-09  grad at x: [-3.35233400e-05  6.22576313e-05]  gradient norm: 7.070945469322654e-05\n",
            "iter: 59  x: [  7.99998659 -11.9999751 ]  f(x): 7.999723172739864e-10  grad at x: [-2.68186720e-05  4.98061051e-05]  gradient norm: 5.656756375429249e-05\n",
            "iter: 60  x: [  7.99998927 -11.99998008]  f(x): 5.119822830798517e-10  grad at x: [-2.14549376e-05  3.98448841e-05]  gradient norm: 4.525405100451678e-05\n",
            "iter: 61  x: [  7.99999142 -11.99998406]  f(x): 3.2766866116587834e-10  grad at x: [-1.71639501e-05  3.18759073e-05]  gradient norm: 3.620324080332468e-05\n",
            "iter: 62  x: [  7.99999313 -11.99998725]  f(x): 2.0970794315766094e-10  grad at x: [-1.37311600e-05  2.55007258e-05]  gradient norm: 2.8962592643453794e-05\n",
            "iter: 63  x: [  7.99999451 -11.9999898 ]  f(x): 1.3421308362619942e-10  grad at x: [-1.09849280e-05  2.04005806e-05]  gradient norm: 2.3170074115220214e-05\n",
            "iter: 64  x: [  7.99999561 -11.99999184]  f(x): 8.589637352968794e-11  grad at x: [-8.78794243e-06  1.63204645e-05]  gradient norm: 1.853605929313865e-05\n",
            "iter: 65  x: [  7.99999648 -11.99999347]  f(x): 5.49736790568594e-11  grad at x: [-7.03035395e-06  1.30563716e-05]  gradient norm: 1.4828847434222175e-05\n",
            "iter: 66  x: [  7.99999719 -11.99999478]  f(x): 3.518315459738909e-11  grad at x: [-5.62428316e-06  1.04450973e-05]  gradient norm: 1.1863077947546175e-05\n",
            "iter: 67  x: [  7.99999775 -11.99999582]  f(x): 2.2517218946096954e-11  grad at x: [-4.49942653e-06  8.35607783e-06]  gradient norm: 9.490462358830986e-06\n",
            "[  7.99999775 -11.99999582]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_opt_els = find_minimizer(my_start_x, my_tol, EXACT_LINE_SEARCH)\n",
        "print(x_opt_els)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVdK8GS0zz8-",
        "outputId": "e310661b-c10b-4068-f173-aa334fb4e2a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0  x: [1 1]  f(x): 218  grad at x: [-14  26]  gradient norm: 29.5296461204668\n",
            "iter: 1  x: [  8. -12.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "[  8. -12.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_opt_bls = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\n",
        "print(x_opt_bls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTEA__IDz2dD",
        "outputId": "99180b8f-abd0-47db-c0b5-f1d194756c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params for Backtracking LS: alpha start: 1 rho: 0.5  gamma: 0.5\n",
            "iter: 0  x: [1 1]  f(x): 218  grad at x: [-14  26]  gradient norm: 29.5296461204668\n",
            "iter: 1  x: [  8. -12.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "[  8. -12.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYJaACtKMIGH"
      },
      "source": [
        "${\\Large\\text{Do not forget to rename the file before submission.}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ans 3:\n",
        "\n",
        "Optimizer : [8,-12]\n",
        "\n",
        "Minimum Function Value : 0"
      ],
      "metadata": {
        "id": "UmvKK7xH0ad-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer(start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  # construct a suitable A matrix for the quadratic function \n",
        "  A = np.array([[1, 0],[0,1]])\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    #print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x,evalf(x),k\n"
      ],
      "metadata": {
        "id": "0BpC8Pz8252c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2_tol = 1e-12\n",
        "q2_start_x = np.array([25,25])"
      ],
      "metadata": {
        "id": "DjwXwHBO0vZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcsCIAntMNdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18d016d-f5c6-4b9b-ef8f-60375cd91ad3"
      },
      "source": [
        "x,f,k = find_minimizer(q2_start_x, q2_tol, EXACT_LINE_SEARCH)\n",
        "print('Iterations taken by Exact Line Search:',k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterations taken by Exact Line Search: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,f,k = find_minimizer(q2_start_x, q2_tol, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\n",
        "print('Iterations taken by Backtracking Line Search:',k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPnFHEkU1KrY",
        "outputId": "3d35f439-314b-4bd6-83b2-148cfe6db58c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterations taken by Backtracking Line Search: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ans 4:\n",
        "\n",
        "We can observe that both the methods terminate and give the result after just one iteration for the given function."
      ],
      "metadata": {
        "id": "XPo-RS0Z1Wbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q3_start_x = np.array([25,25])\n",
        "q3_tol = 1e-10\n",
        "alpha_0 = [1, 0.9, 0.75, 0.6, 0.5, 0.4, 0.25, 0.1, 0.01]\n",
        "k_els = []\n",
        "k_bls = []\n",
        "x_opt = []\n",
        "f_min = []\n",
        "for i in alpha_0:\n",
        "  a_els,b_els,c_els = find_minimizer(q3_start_x, q3_tol, EXACT_LINE_SEARCH)\n",
        "  a_bls,b_bls,c_bls = find_minimizer(q3_start_x, q3_tol, BACKTRACKING_LINE_SEARCH, i, 0.5,0.5)\n",
        "  x_opt.append(a_bls)\n",
        "  f_min.append(b_bls)\n",
        "  k_els.append(c_els)\n",
        "  k_bls.append(c_bls)"
      ],
      "metadata": {
        "id": "udN7N8Jc1Rc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(alpha_0,k_bls)\n",
        "plt.xlabel('Alpha_0')\n",
        "plt.ylabel('Iterations')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "C0DKeyt431WW",
        "outputId": "9420e884-6128-45cd-f1a0-48af3fd1c97a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Iterations')"
            ]
          },
          "metadata": {},
          "execution_count": 124
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(alpha_0)):\n",
        "  print('For alpha_0 =',alpha_0[i],'\\nFinal Optimizer :',x_opt[i],'\\nFinal Function Value :',f_min[i],'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC6eRE7e4cJb",
        "outputId": "d5bc8ca6-e885-40c6-b603-8c16463d8108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For alpha_0 = 1 \n",
            "Final Optimizer : [  8. -12.] \n",
            "Final Function Value : 0.0 \n",
            "\n",
            "For alpha_0 = 0.9 \n",
            "Final Optimizer : [  8. -12.] \n",
            "Final Function Value : 1.6579714975258972e-21 \n",
            "\n",
            "For alpha_0 = 0.75 \n",
            "Final Optimizer : [  8. -12.] \n",
            "Final Function Value : 1.3714654556129199e-21 \n",
            "\n",
            "For alpha_0 = 0.6 \n",
            "Final Optimizer : [  8. -12.] \n",
            "Final Function Value : 2.2038291998576117e-21 \n",
            "\n",
            "For alpha_0 = 0.5 \n",
            "Final Optimizer : [  8. -12.] \n",
            "Final Function Value : 0.0 \n",
            "\n",
            "For alpha_0 = 0.4 \n",
            "Final Optimizer : [  8. -12.] \n",
            "Final Function Value : 1.1393259623274523e-22 \n",
            "\n",
            "For alpha_0 = 0.25 \n",
            "Final Optimizer : [  8. -12.] \n",
            "Final Function Value : 1.3714654556129199e-21 \n",
            "\n",
            "For alpha_0 = 0.1 \n",
            "Final Optimizer : [  8. -12.] \n",
            "Final Function Value : 2.3972320602008796e-21 \n",
            "\n",
            "For alpha_0 = 0.01 \n",
            "Final Optimizer : [  8. -12.] \n",
            "Final Function Value : 2.4523367712209537e-21 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(alpha_0)):\n",
        "  print('For alpha_0:',alpha_0[i],'\\nNo. of iterations used by Exact Line Search:',k_els[i],'\\nNo. of iterations used by Backtracking Line Search:',k_bls[i],'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6thfTmyD-LAO",
        "outputId": "f275f7f9-ec52-45b5-8585-539016598e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For alpha_0: 1 \n",
            "No. of iterations used by Exact Line Search: 1 \n",
            "No. of iterations used by Backtracking Line Search: 1 \n",
            "\n",
            "For alpha_0: 0.9 \n",
            "No. of iterations used by Exact Line Search: 1 \n",
            "No. of iterations used by Backtracking Line Search: 12 \n",
            "\n",
            "For alpha_0: 0.75 \n",
            "No. of iterations used by Exact Line Search: 1 \n",
            "No. of iterations used by Backtracking Line Search: 20 \n",
            "\n",
            "For alpha_0: 0.6 \n",
            "No. of iterations used by Exact Line Search: 1 \n",
            "No. of iterations used by Backtracking Line Search: 30 \n",
            "\n",
            "For alpha_0: 0.5 \n",
            "No. of iterations used by Exact Line Search: 1 \n",
            "No. of iterations used by Backtracking Line Search: 1 \n",
            "\n",
            "For alpha_0: 0.4 \n",
            "No. of iterations used by Exact Line Search: 1 \n",
            "No. of iterations used by Backtracking Line Search: 18 \n",
            "\n",
            "For alpha_0: 0.25 \n",
            "No. of iterations used by Exact Line Search: 1 \n",
            "No. of iterations used by Backtracking Line Search: 40 \n",
            "\n",
            "For alpha_0: 0.1 \n",
            "No. of iterations used by Exact Line Search: 1 \n",
            "No. of iterations used by Backtracking Line Search: 123 \n",
            "\n",
            "For alpha_0: 0.01 \n",
            "No. of iterations used by Exact Line Search: 1 \n",
            "No. of iterations used by Backtracking Line Search: 1358 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ans 5:\n",
        "\n",
        "We can observe from the plot between iterations taken by backtracking line search and $\\alpha^{0}$ that as the value of $\\alpha^{0}$ reduces the no. of iterations increases rapidly.\n",
        "\n",
        "\n",
        "We can observe that except for when $\\alpha^{0}$ = 1 and 0.5, the minimum function value is away from 0 by a factor of $10^{-21}$.\n",
        "\n",
        "We can observe that backtracking line search never takes lesser iterations than exact line search and only equals the no. of iterations as that of exact line search when $\\alpha^{0} = \\{1,0.5\\}$."
      ],
      "metadata": {
        "id": "Cex13hRS5RwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q4_start_x = np.array([25,25])\n",
        "q4_tol = 1e-10\n",
        "rho = [0.9, 0.75, 0.6, 0.5, 0.4, 0.25, 0.1, 0.01]\n",
        "k_els = []\n",
        "k_bls = []\n",
        "x_opt = []\n",
        "f_min = []\n",
        "for i in rho:\n",
        "  a_els,b_els,c_els = find_minimizer(q3_start_x, q3_tol, EXACT_LINE_SEARCH)\n",
        "  a_bls,b_bls,c_bls = find_minimizer(q3_start_x, q3_tol, BACKTRACKING_LINE_SEARCH, 1, i,0.5)\n",
        "  x_opt.append(a_bls)\n",
        "  f_min.append(b_bls)\n",
        "  k_els.append(c_els)\n",
        "  k_bls.append(c_bls)"
      ],
      "metadata": {
        "id": "iDR3hHH65Lnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(rho,k_bls)\n",
        "plt.xlabel('Rho')\n",
        "plt.ylabel('Iterations')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "iyjcJdyaAXoR",
        "outputId": "d66168d7-de90-45a6-898a-5791f1936f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Iterations')"
            ]
          },
          "metadata": {},
          "execution_count": 128
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(rho)):\n",
        "  print('For rho =',rho[i],'\\nFinal Optimizer :',x_opt[i],'\\nFinal Function Value :',f_min[i],'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvHK_CYQAo2j",
        "outputId": "e9d9f3fc-f21c-45ff-d2d4-43a57b79d424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For rho = 0.9 \n",
            "Final Optimizer : [  8. -12.] \n",
            "Final Function Value : 4.960536411900771e-22 \n",
            "\n",
            "For rho = 0.75 \n",
            "Final Optimizer : [  8. -12.] \n",
            "Final Function Value : 1.0819468296335504e-21 \n",
            "\n",
            "For rho = 0.6 \n",
            "Final Optimizer : [  8. -12.] \n",
            "Final Function Value : 7.844395544174143e-22 \n",
            "\n",
            "For rho = 0.5 \n",
            "Final Optimizer : [  8. -12.] \n",
            "Final Function Value : 0.0 \n",
            "\n",
            "For rho = 0.4 \n",
            "Final Optimizer : [  8. -12.] \n",
            "Final Function Value : 1.1393259623274523e-22 \n",
            "\n",
            "For rho = 0.25 \n",
            "Final Optimizer : [  8. -12.] \n",
            "Final Function Value : 1.3714654556129199e-21 \n",
            "\n",
            "For rho = 0.1 \n",
            "Final Optimizer : [  8. -12.] \n",
            "Final Function Value : 2.3972320602008796e-21 \n",
            "\n",
            "For rho = 0.01 \n",
            "Final Optimizer : [  8. -12.] \n",
            "Final Function Value : 2.4523367712209537e-21 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(rho)):\n",
        "  print('For rho:',rho[i],'\\nNo. of iterations used by Exact Line Search:',k_els[i],'\\nNo. of iterations used by Backtracking Line Search:',k_bls[i],'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J_DFrGMA6ag",
        "outputId": "0b65f767-1fb2-4d4e-ed94-7730772fd64e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For rho: 0.9 \n",
            "No. of iterations used by Exact Line Search: 1 \n",
            "No. of iterations used by Backtracking Line Search: 9 \n",
            "\n",
            "For rho: 0.75 \n",
            "No. of iterations used by Exact Line Search: 1 \n",
            "No. of iterations used by Backtracking Line Search: 15 \n",
            "\n",
            "For rho: 0.6 \n",
            "No. of iterations used by Exact Line Search: 1 \n",
            "No. of iterations used by Backtracking Line Search: 22 \n",
            "\n",
            "For rho: 0.5 \n",
            "No. of iterations used by Exact Line Search: 1 \n",
            "No. of iterations used by Backtracking Line Search: 1 \n",
            "\n",
            "For rho: 0.4 \n",
            "No. of iterations used by Exact Line Search: 1 \n",
            "No. of iterations used by Backtracking Line Search: 18 \n",
            "\n",
            "For rho: 0.25 \n",
            "No. of iterations used by Exact Line Search: 1 \n",
            "No. of iterations used by Backtracking Line Search: 40 \n",
            "\n",
            "For rho: 0.1 \n",
            "No. of iterations used by Exact Line Search: 1 \n",
            "No. of iterations used by Backtracking Line Search: 123 \n",
            "\n",
            "For rho: 0.01 \n",
            "No. of iterations used by Exact Line Search: 1 \n",
            "No. of iterations used by Backtracking Line Search: 1358 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ans 6:\n",
        "\n",
        "We can observe from the plot between iterations taken by backtracking line search and $\\rho$ that as the value of $\\rho$ reduces the no. of iterations increases rapidly.\n",
        "\n",
        "\n",
        "We can observe that except for when $\\rho$ = 0.5, the minimum function value is away from 0 by a factor of $10^{-21}$.\n",
        "\n",
        "We can observe that backtracking line search never takes lesser iterations than exact line search and only equals the no. of iterations as that of exact line search when $\\rho = 0.5$."
      ],
      "metadata": {
        "id": "Lt9INnDBBVle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lx_-m5U0BHl6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}