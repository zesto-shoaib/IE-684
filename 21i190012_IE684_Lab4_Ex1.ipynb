{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21i190012_IE684_Lab4_Ex1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3BUcB8dfw0ge"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import sqrtm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evalf(x):\n",
        "  assert type(x) is np.ndarray and len(x) == 2\n",
        "  return 400*x[0]**2 + 0.004*x[1]**4"
      ],
      "metadata": {
        "id": "MKkSRKV9w_pN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalg(x):\n",
        "  assert type(x) is np.ndarray and len(x) == 2\n",
        "  return np.array([800*x[0],0.016*x[1]**3])"
      ],
      "metadata": {
        "id": "4xisygLdxC8i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalh(x):\n",
        "  assert type(x) is np.ndarray and len(x) == 2\n",
        "  return np.array([[800,0],[0,0.048*x[1]**2]])"
      ],
      "metadata": {
        "id": "0FjavhMGxpk7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer(start_x, tol):\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2\n",
        "  assert type(tol) is float and tol>=0\n",
        "\n",
        "  x = start_x\n",
        "  gradf = evalg(x)\n",
        "  hesf = evalh(x)\n",
        "  step_length = 1\n",
        "  k = 0\n",
        "\n",
        "  while np.linalg.norm(gradf) > tol:\n",
        "    x = np.subtract(x,np.multiply(step_length,np.matmul(np.linalg.inv(hesf),gradf)))\n",
        "    k += 1\n",
        "    gradf = evalg(x)\n",
        "    hesf = evalh(x)\n",
        "    print('x:',x)\n",
        "\n",
        "  return x, k, evalf(x)"
      ],
      "metadata": {
        "id": "EiJMocqF6p-X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_x = np.array([2,2])\n",
        "tol = 1e-9"
      ],
      "metadata": {
        "id": "dM287Ob2Rn1b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_minimizer(start_x, tol)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4pZ0s278YFi",
        "outputId": "879e0ff3-df8b-497d-b3af-0179c4f11ed6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [0.         1.33333333]\n",
            "x: [0.         0.88888889]\n",
            "x: [0.         0.59259259]\n",
            "x: [0.         0.39506173]\n",
            "x: [0.         0.26337449]\n",
            "x: [0.         0.17558299]\n",
            "x: [0.         0.11705533]\n",
            "x: [0.         0.07803688]\n",
            "x: [0.         0.05202459]\n",
            "x: [0.         0.03468306]\n",
            "x: [0.         0.02312204]\n",
            "x: [0.         0.01541469]\n",
            "x: [0.         0.01027646]\n",
            "x: [0.         0.00685097]\n",
            "x: [0.         0.00456732]\n",
            "x: [0.         0.00304488]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.        , 0.00304488]), 16, 3.4382653805813626e-13)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steplength_backtracking_scaled_direction(x, gradf, alpha_start, rho, gamma, d_k):\n",
        "  assert type(x) is np.ndarray and len(x) == 2\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2\n",
        "  \n",
        "  alpha = alpha_start\n",
        "\n",
        "  while evalf(x+alpha*np.matmul(d_k,-gradf)) > evalf(x) + gamma*alpha*(np.matmul(gradf.transpose(),np.matmul(d_k,-gradf))):\n",
        "    alpha = rho*alpha\n",
        "\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "ufLJ9CYw8g2q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer_newton_backtracking(start_x, tol, *args):\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2\n",
        "  assert type(tol) is float and tol>=0\n",
        "\n",
        "  x = start_x\n",
        "  gradf = evalg(x)\n",
        "  hesf = evalh(x)\n",
        "\n",
        "  alpha_start = args[0]\n",
        "  rho = args[1]\n",
        "  gamma = args[2]\n",
        "\n",
        "  dk = np.linalg.inv(hesf)\n",
        "\n",
        "  k = 0\n",
        "\n",
        "  while np.linalg.norm(gradf) > tol:\n",
        "    \n",
        "    step_length = compute_steplength_backtracking_scaled_direction(x,gradf,alpha_start,rho,gamma,dk)\n",
        "    x = np.subtract(x,np.multiply(step_length,np.matmul(dk,gradf)))\n",
        "\n",
        "    gradf = evalg(x)\n",
        "    hesf = evalh(x)\n",
        "    k += 1\n",
        "    dk = np.linalg.inv(hesf)\n",
        "    print('x:',x,'alpha:',step_length)\n",
        "\n",
        "  return x,k,evalf(x)"
      ],
      "metadata": {
        "id": "-1xCoNvW9l_V"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_minimizer_newton_backtracking(start_x, tol, 1.0, 0.5, 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLZ2dYZU_OuO",
        "outputId": "62e9ced3-55a4-4bbb-fb41-383667f7a01c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [0.         1.33333333] alpha: 1.0\n",
            "x: [0.         0.88888889] alpha: 1.0\n",
            "x: [0.         0.59259259] alpha: 1.0\n",
            "x: [0.         0.39506173] alpha: 1.0\n",
            "x: [0.         0.26337449] alpha: 1.0\n",
            "x: [0.         0.17558299] alpha: 1.0\n",
            "x: [0.         0.11705533] alpha: 1.0\n",
            "x: [0.         0.07803688] alpha: 1.0\n",
            "x: [0.         0.05202459] alpha: 1.0\n",
            "x: [0.         0.03468306] alpha: 1.0\n",
            "x: [0.         0.02312204] alpha: 1.0\n",
            "x: [0.         0.01541469] alpha: 1.0\n",
            "x: [0.         0.01027646] alpha: 1.0\n",
            "x: [0.         0.00685097] alpha: 1.0\n",
            "x: [0.         0.00456732] alpha: 1.0\n",
            "x: [0.         0.00304488] alpha: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.        , 0.00304488]), 16, 3.4382653805813626e-13)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ans 3:\n",
        "\n",
        "For Newton's Method with step length = 1, we get the optimizer in 16 iterations.\n",
        "\n",
        "For Newton's Method with Backtracking Line Search, we get the optimizer in 16 iterations.\n",
        "\n",
        "Both the methods take same no. of iterations.\n",
        "\n",
        "Minimizer for Newton's Method : [0.        , 0.00304488]\n",
        "\n",
        "Minimum Function Value for Newton's Method : 3.4382653805813626e-13\n",
        "\n",
        "Minimizer for Newton's Method with backtracking line search : [0.        , 0.00304488]\n",
        "\n",
        "Minimum function value for Newton's method with backtracking line search : 3.4382653805813626e-13\n",
        "\n",
        "Both the methods give same minimizer and minimum function value.\n"
      ],
      "metadata": {
        "id": "N2jXjpssoZA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steplength_backtracking_prev(x, gradf, alpha_start, rho, gamma):\n",
        "  assert type(x) is np.ndarray and len(x) == 2\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2\n",
        "  \n",
        "  alpha = alpha_start\n",
        "\n",
        "  while evalf(x+alpha*-gradf) > evalf(x) + gamma*alpha*np.matmul(gradf.transpose(),-gradf):\n",
        "    alpha = rho*alpha\n",
        "\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "b5stHXUi_hjr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steplength_backtracking_scaled_direction_prev(x, gradf, alpha_start, rho, gamma, d_k):\n",
        "  assert type(x) is np.ndarray and len(x) == 2\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2\n",
        "  \n",
        "  alpha = alpha_start\n",
        "\n",
        "  while evalf(x+alpha*np.matmul(d_k,(-gradf))) > evalf(x) - gamma*alpha*np.matmul(gradf.transpose(),np.matmul(d_k,-gradf)):\n",
        "    alpha = rho*alpha\n",
        "\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "ddZV4R9tpSbi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BACKTRACKING_LINE_SEARCH = 1\n",
        "BACKTRACKING_LINE_SEARCH_WITH_SCALING = 2"
      ],
      "metadata": {
        "id": "rOwZjFSFpStP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer_prev(start_x, tol, line_search_type, *args):\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2\n",
        "  assert type(tol) is float and tol>=0 \n",
        "\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "\n",
        "  alpha_start = args[0]\n",
        "  rho = args[1]\n",
        "  gamma = args[2]\n",
        "  hes_x = evalh(x)\n",
        "\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH_WITH_SCALING):\n",
        "    d_k = np.linalg.inv(hes_x)\n",
        "\n",
        "  k = 0\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol):\n",
        "  \n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking_prev(x,g_x, alpha_start,rho, gamma)\n",
        "      x = np.subtract(x, np.multiply(step_length,g_x))\n",
        "      k += 1 \n",
        "      print('x:',x,'alpha:',step_length)\n",
        "      g_x = evalg(x) \n",
        "      hes_x = evalh(x)\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH_WITH_SCALING:\n",
        "      step_length = compute_steplength_backtracking_scaled_direction_prev(x,g_x, alpha_start,rho, gamma, d_k)\n",
        "      x = np.subtract(x, np.multiply(step_length,np.matmul(d_k,g_x))) \n",
        "      k += 1\n",
        "      print('x:',x,'alpha:',step_length)\n",
        "      g_x = evalg(x) \n",
        "      hes_x = evalh(x)\n",
        "      d_k = np.linalg.inv(hes_x) \n",
        "      \n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x,k,evalf(x)"
      ],
      "metadata": {
        "id": "WQy0jvsfpVac"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_minimizer_prev(start_x, tol, BACKTRACKING_LINE_SEARCH,1.,.5,.5)"
      ],
      "metadata": {
        "id": "TJab-FeiWfSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_minimizer_prev(start_x,tol,BACKTRACKING_LINE_SEARCH_WITH_SCALING,1.,.5,.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYGEzLJkp7SY",
        "outputId": "c91b827e-2c1b-4131-e862-c544a447a13d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [0.         1.33333333] alpha: 1.0\n",
            "x: [0.         0.88888889] alpha: 1.0\n",
            "x: [0.         0.59259259] alpha: 1.0\n",
            "x: [0.         0.39506173] alpha: 1.0\n",
            "x: [0.         0.26337449] alpha: 1.0\n",
            "x: [0.         0.17558299] alpha: 1.0\n",
            "x: [0.         0.11705533] alpha: 1.0\n",
            "x: [0.         0.07803688] alpha: 1.0\n",
            "x: [0.         0.05202459] alpha: 1.0\n",
            "x: [0.         0.03468306] alpha: 1.0\n",
            "x: [0.         0.02312204] alpha: 1.0\n",
            "x: [0.         0.01541469] alpha: 1.0\n",
            "x: [0.         0.01027646] alpha: 1.0\n",
            "x: [0.         0.00685097] alpha: 1.0\n",
            "x: [0.         0.00456732] alpha: 1.0\n",
            "x: [0.         0.00304488] alpha: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.        , 0.00304488]), 16, 3.4382653805813626e-13)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ans 4:\n",
        "\n",
        "For Newton's Method with step length = 1, we get the optimizer in 16 iterations.\n",
        "\n",
        "For Newton's Method with Backtracking Line Search, we get the optimizer in 16 iterations.\n",
        "\n",
        "For Gradient Descent without backtracking line search, it does not converge quickly, so I stopped it midway.\n",
        "\n",
        "For Gradient Descent with backtracking line search, we get the optimizer in 16 iterations.\n",
        "\n",
        "All the methods except gradient descent without backtracking line search take same no. of iterations.\n",
        "\n",
        "Minimizer for Newton's Method : [0.        , 0.00304488]\n",
        "\n",
        "Minimum Function Value for Newton's Method : 3.4382653805813626e-13\n",
        "\n",
        "Minimizer for Newton's Method with backtracking line search : [0.        , 0.00304488]\n",
        "\n",
        "Minimum function value for Newton's method with backtracking line search : 3.4382653805813626e-13\n",
        "\n",
        "Minimizer for Gradient Descent without backtracking line search : [0.        , 0.00304488]\n",
        "\n",
        "Minimum function Value for Gradient descent without backtracking line search : 3.4382653805813626e-13\n",
        "\n",
        "All the methods except Gradient Descent without backtracking line search give same minimizer and minimum function value.\n"
      ],
      "metadata": {
        "id": "Ulfag_i-Wg-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "h0wcZqQTS27l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}