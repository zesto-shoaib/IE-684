{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21i190012_IE684_Lab4_Ex2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZASVgKg0qXyK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import sqrtm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evalf(x):\n",
        "  assert type(x) is np.ndarray and len(x) == 2\n",
        "  return np.sqrt(x[0]**2+4) + np.sqrt(x[1]**2+4)"
      ],
      "metadata": {
        "id": "b4XOxtVM_hV7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalg(x):\n",
        "  assert type(x) is np.ndarray and len(x) == 2\n",
        "  return np.array([x[0]/np.sqrt(x[0]**2+4),x[1]/np.sqrt(x[1]**2+4)])"
      ],
      "metadata": {
        "id": "MVJ7ZVSe_3B2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalh(x):\n",
        "  assert type(x) is np.ndarray and len(x) == 2\n",
        "  return np.array([[4/np.power(x[0]**2+4,3/2),0],[0,4/np.power(x[1]**2+4,3/2)]])"
      ],
      "metadata": {
        "id": "jnJveHMD_6T1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer(start_x, tol):\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2\n",
        "  assert type(tol) is float and tol>=0\n",
        "\n",
        "  x = start_x\n",
        "  gradf = evalg(x)\n",
        "  hesf = evalh(x)\n",
        "  step_length = 1\n",
        "  k = 0\n",
        "\n",
        "  while np.linalg.norm(gradf) > tol:\n",
        "    x = np.subtract(x,np.multiply(step_length,np.matmul(np.linalg.inv(hesf),gradf)))\n",
        "    k += 1\n",
        "    gradf = evalg(x)\n",
        "    hesf = evalh(x)\n",
        "    print('x:',x)\n",
        "\n",
        "  return x, k, evalf(x)"
      ],
      "metadata": {
        "id": "_6jBfa5qDpJ1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_x = np.array([2,2])\n",
        "tol = 1e-9"
      ],
      "metadata": {
        "id": "cmodfOWHD4bQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_minimizer(start_x,tol)"
      ],
      "metadata": {
        "id": "ODgU870_LAjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steplength_backtracking_scaled_direction(x, gradf, alpha_start, rho, gamma, d_k):\n",
        "  assert type(x) is np.ndarray and len(x) == 2\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2\n",
        "  \n",
        "  alpha = alpha_start\n",
        "\n",
        "  while evalf(x+alpha*np.matmul(d_k,-gradf)) > evalf(x) + gamma*alpha*(np.matmul(gradf.transpose(),np.matmul(d_k,-gradf))):\n",
        "    alpha = rho*alpha\n",
        "\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "xHYpgyRzEAGi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer_newton_backtracking(start_x, tol, *args):\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2\n",
        "  assert type(tol) is float and tol>=0\n",
        "\n",
        "  x = start_x\n",
        "  gradf = evalg(x)\n",
        "  hesf = evalh(x)\n",
        "\n",
        "  alpha_start = args[0]\n",
        "  rho = args[1]\n",
        "  gamma = args[2]\n",
        "\n",
        "  dk = np.linalg.inv(hesf)\n",
        "\n",
        "  k = 0\n",
        "\n",
        "  while np.linalg.norm(gradf) > tol:\n",
        "    \n",
        "    step_length = compute_steplength_backtracking_scaled_direction(x,gradf,alpha_start,rho,gamma,dk)\n",
        "    x = np.subtract(x,np.multiply(step_length,np.matmul(dk,gradf)))\n",
        "\n",
        "    gradf = evalg(x)\n",
        "    hesf = evalh(x)\n",
        "    k += 1\n",
        "    dk = np.linalg.inv(hesf)\n",
        "    print('x:',x,'alpha:',step_length)\n",
        "\n",
        "  return x,k,evalf(x)"
      ],
      "metadata": {
        "id": "cIyicf6yEYfo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_minimizer_newton_backtracking(start_x, tol, 1.0, 0.5, 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKR6XAwkEmKZ",
        "outputId": "e7314d54-8f72-431f-b18f-1b3eea503e39"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [0. 0.] alpha: 0.5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 0.]), 1, 4.0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ans 2:\n",
        "\n",
        "We can observe that Newton's Method with constant step length = 1 is diverging and oscillates between two values.\n",
        "\n",
        "As for Newton's Method with Backtracking Line Search, it takes 1 iteration to find optimizer.\n",
        "\n",
        "Minimizer : [0,0]\n",
        "\n",
        "Minimum Function Value : 4.0"
      ],
      "metadata": {
        "id": "GUx7wIY-Gr7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steplength_backtracking_prev(x, gradf, alpha_start, rho, gamma):\n",
        "  assert type(x) is np.ndarray and len(x) == 2\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2\n",
        "  \n",
        "  alpha = alpha_start\n",
        "\n",
        "  while evalf(x+alpha*-gradf) > evalf(x) + gamma*alpha*np.matmul(gradf.transpose(),-gradf):\n",
        "    alpha = rho*alpha\n",
        "\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "dzJpdxEoEuR2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer_prev(start_x, tol,*args):\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2\n",
        "  assert type(tol) is float and tol>=0 \n",
        "\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "\n",
        "  alpha_start = args[0]\n",
        "  rho = args[1]\n",
        "  gamma = args[2]\n",
        "  hes_x = evalh(x)\n",
        "\n",
        "  k = 0\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol):\n",
        "\n",
        "    step_length = compute_steplength_backtracking_prev(x,g_x, alpha_start,rho, gamma)\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x))\n",
        "    k += 1 \n",
        "    print('x:',x,'alpha:',step_length)\n",
        "    g_x = evalg(x) \n",
        "    hes_x = evalh(x)\n",
        "    \n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x,k,evalf(x)"
      ],
      "metadata": {
        "id": "YETthDwEIJl5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_minimizer_prev(start_x, tol, 1.0, 0.5, 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JleNrAoVIL6i",
        "outputId": "2b6ae058-6522-427a-eff9-d9125a7e4745"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [1.29289322 1.29289322] alpha: 1.0\n",
            "x: [0.7500044 0.7500044] alpha: 1.0\n",
            "x: [0.39887915 0.39887915] alpha: 1.0\n",
            "x: [0.20329151 0.20329151] alpha: 1.0\n",
            "x: [0.10216681 0.10216681] alpha: 1.0\n",
            "x: [0.05114993 0.05114993] alpha: 1.0\n",
            "x: [0.02558332 0.02558332] alpha: 1.0\n",
            "x: [0.01279271 0.01279271] alpha: 1.0\n",
            "x: [0.00639649 0.00639649] alpha: 1.0\n",
            "x: [0.00319826 0.00319826] alpha: 1.0\n",
            "x: [0.00159913 0.00159913] alpha: 1.0\n",
            "x: [0.00079957 0.00079957] alpha: 1.0\n",
            "x: [0.00039978 0.00039978] alpha: 1.0\n",
            "x: [0.00019989 0.00019989] alpha: 1.0\n",
            "x: [9.99457603e-05 9.99457603e-05] alpha: 1.0\n",
            "x: [4.99728802e-05 4.99728802e-05] alpha: 1.0\n",
            "x: [2.49864401e-05 2.49864401e-05] alpha: 1.0\n",
            "x: [1.24932201e-05 1.24932201e-05] alpha: 1.0\n",
            "x: [6.24661003e-06 6.24661003e-06] alpha: 1.0\n",
            "x: [3.12330501e-06 3.12330501e-06] alpha: 1.0\n",
            "x: [1.56165251e-06 1.56165251e-06] alpha: 1.0\n",
            "x: [7.80826253e-07 7.80826253e-07] alpha: 1.0\n",
            "x: [3.90413127e-07 3.90413127e-07] alpha: 1.0\n",
            "x: [1.95206563e-07 1.95206563e-07] alpha: 1.0\n",
            "x: [9.76032817e-08 9.76032817e-08] alpha: 1.0\n",
            "x: [4.88016408e-08 4.88016408e-08] alpha: 1.0\n",
            "x: [2.44008204e-08 2.44008204e-08] alpha: 1.0\n",
            "x: [1.22004102e-08 1.22004102e-08] alpha: 1.0\n",
            "x: [6.1002051e-09 6.1002051e-09] alpha: 1.0\n",
            "x: [3.05010255e-09 3.05010255e-09] alpha: 1.0\n",
            "x: [1.52505128e-09 1.52505128e-09] alpha: 1.0\n",
            "x: [7.62525638e-10 7.62525638e-10] alpha: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([7.62525638e-10, 7.62525638e-10]), 32, 4.0)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ans 3:\n",
        "\n",
        "We can observe that Newton's Method with constant step length = 1 is diverging and oscillates between two values.\n",
        "\n",
        "As for Newton's Method with Backtracking Line Search, it takes 1 iteration to find optimizer.\n",
        "\n",
        "In Gradient Descent with backtracking line search, it takes 32 iterations.\n",
        "\n",
        "Newton's Method with Backtracking Line Search takes way less iterations than Gradient Descent with Backtracking Line Search.\n",
        "\n",
        "Minimizer for Newton's Method with Backtracking Line Search : [0,0]\n",
        "\n",
        "Minimum Function Value for Newton's Method with Backtracking Line Search : 4.0\n",
        "\n",
        "Minimizer for Gradient Descent with backtracking line search : [7.62525638e-10, 7.62525638e-10]\n",
        "\n",
        "Minimizer for Gradient Descent with backtracking line search : 4.0\n",
        "\n",
        "The minimizer is equal to the actual minimizer for Newton's method with Backtracking Line Search, but it is approximately equal in the case of Gradient Descent with Backtracking Line Search.\n",
        "\n",
        "The minimum function value is same in both cases.\n"
      ],
      "metadata": {
        "id": "X4WyZAgQLxTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_x_4 = np.array([8.,8.])"
      ],
      "metadata": {
        "id": "Y0qgSrxAJdCz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_minimizer(start_x_4, tol)"
      ],
      "metadata": {
        "id": "tTBx_YhOM__H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_minimizer_newton_backtracking(start_x_4, tol, 1.0, 0.5, 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdtEoFsNJkBe",
        "outputId": "dd698435-12b4-46b7-8cd1-15f9e4f5e1a2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [-0.5 -0.5] alpha: 0.0625\n",
            "x: [-0.234375 -0.234375] alpha: 0.5\n",
            "x: [-0.11557817 -0.11557817] alpha: 0.5\n",
            "x: [-0.0575961 -0.0575961] alpha: 0.5\n",
            "x: [-0.02877417 -0.02877417] alpha: 0.5\n",
            "x: [-0.0143841 -0.0143841] alpha: 0.5\n",
            "x: [-0.00719168 -0.00719168] alpha: 0.5\n",
            "x: [-0.00359579 -0.00359579] alpha: 0.5\n",
            "x: [-0.00179789 -0.00179789] alpha: 0.5\n",
            "x: [-0.00089894 -0.00089894] alpha: 0.5\n",
            "x: [-0.00044947 -0.00044947] alpha: 0.5\n",
            "x: [-0.00022474 -0.00022474] alpha: 0.5\n",
            "x: [2.83764947e-12 2.83764947e-12] alpha: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2.83764947e-12, 2.83764947e-12]), 13, 4.0)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ans 4:\n",
        "\n",
        "We can observe that Newton's Method with constant step length = 1, it is diverging and approaches infinity.\n",
        "\n",
        "As for Newton's Method with Backtracking Line Search, it takes 13 iterations to find optimizer.\n",
        "\n",
        "Minimizer : [2.83764947e-12, 2.83764947e-12]\n",
        "\n",
        "Minimum Function Value : 4.0"
      ],
      "metadata": {
        "id": "si1l6RxpNMup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "find_minimizer_prev(start_x_4, tol, 1.0, 0.5, 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gUpn_lxJ7H0",
        "outputId": "dc64f3e7-92cb-4d4f-c2ee-3cbc3acd3b77"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [7.0298575 7.0298575] alpha: 1.0\n",
            "x: [6.06802585 6.06802585] alpha: 1.0\n",
            "x: [5.11828321 5.11828321] alpha: 1.0\n",
            "x: [4.18686723 4.18686723] alpha: 1.0\n",
            "x: [3.28453054 3.28453054] alpha: 1.0\n",
            "x: [2.43041522 2.43041522] alpha: 1.0\n",
            "x: [1.6582483 1.6582483] alpha: 1.0\n",
            "x: [1.01997818 1.01997818] alpha: 1.0\n",
            "x: [0.56565981 0.56565981] alpha: 1.0\n",
            "x: [0.2935057 0.2935057] alpha: 1.0\n",
            "x: [0.14830804 0.14830804] alpha: 1.0\n",
            "x: [0.07435706 0.07435706] alpha: 1.0\n",
            "x: [0.0372042 0.0372042] alpha: 1.0\n",
            "x: [0.01860532 0.01860532] alpha: 1.0\n",
            "x: [0.00930306 0.00930306] alpha: 1.0\n",
            "x: [0.00465158 0.00465158] alpha: 1.0\n",
            "x: [0.0023258 0.0023258] alpha: 1.0\n",
            "x: [0.0011629 0.0011629] alpha: 1.0\n",
            "x: [0.00058145 0.00058145] alpha: 1.0\n",
            "x: [0.00029072 0.00029072] alpha: 1.0\n",
            "x: [0.00014536 0.00014536] alpha: 1.0\n",
            "x: [7.26812126e-05 7.26812126e-05] alpha: 1.0\n",
            "x: [3.63406063e-05 3.63406063e-05] alpha: 1.0\n",
            "x: [1.81703032e-05 1.81703032e-05] alpha: 1.0\n",
            "x: [9.08515158e-06 9.08515158e-06] alpha: 1.0\n",
            "x: [4.54257579e-06 4.54257579e-06] alpha: 1.0\n",
            "x: [2.2712879e-06 2.2712879e-06] alpha: 1.0\n",
            "x: [1.13564395e-06 1.13564395e-06] alpha: 1.0\n",
            "x: [5.67821974e-07 5.67821974e-07] alpha: 1.0\n",
            "x: [2.83910987e-07 2.83910987e-07] alpha: 1.0\n",
            "x: [1.41955493e-07 1.41955493e-07] alpha: 1.0\n",
            "x: [7.09777467e-08 7.09777467e-08] alpha: 1.0\n",
            "x: [3.54888734e-08 3.54888734e-08] alpha: 1.0\n",
            "x: [2.6616655e-08 2.6616655e-08] alpha: 0.5\n",
            "x: [1.33083275e-08 1.33083275e-08] alpha: 1.0\n",
            "x: [6.65416376e-09 6.65416376e-09] alpha: 1.0\n",
            "x: [3.32708188e-09 3.32708188e-09] alpha: 1.0\n",
            "x: [1.66354094e-09 1.66354094e-09] alpha: 1.0\n",
            "x: [8.3177047e-10 8.3177047e-10] alpha: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([8.3177047e-10, 8.3177047e-10]), 39, 4.0)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ans 5:\n",
        "\n",
        "We can observe that Newton's Method with constant step length = 1 is diverging and approaches infinity.\n",
        "\n",
        "As for Newton's Method with Backtracking Line Search, it takes 13 iterations to find optimizer.\n",
        "\n",
        "In Gradient Descent with backtracking line search, it takes 39 iterations.\n",
        "\n",
        "Gradient Descent with Backtracking Line Search takes 3 times more iterations than Newton's Method with Backtracking Line Search.\n",
        "\n",
        "Minimizer for Newton's Method with Backtracking Line Search : [2.83764947e-12, 2.83764947e-12]\n",
        "\n",
        "Minimum Function Value for Newton's Method with Backtracking Line Search : 4.0\n",
        "\n",
        "Minimizer for Gradient Descent with backtracking line search : [8.3177047e-10, 8.3177047e-10]\n",
        "\n",
        "Minimizer for Gradient Descent with backtracking line search : 4.0\n",
        "\n",
        "\n",
        "\n",
        "For both cases, the minimizers are approximately equal to the actual minimizer. The minimizer for Newton's method with Backtracking Line Search is better than that in the case of Gradient Descent with Backtracking Line Search with a factor of $10^{-2}$\n",
        "\n",
        "The minimum function value is same in both cases.\n"
      ],
      "metadata": {
        "id": "e73f5meGOUcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nHHW9XyrKnan"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}